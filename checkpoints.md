# Общий план работы над годовым проектом по теме: “Прогнозирование цен акций”


### 1. Разведочный анализ данных (Exploratory Data Analysis, EDA)
**Ответственные:** Все участники команды
**Сроки:** Октябрь 2024 – Ноябрь 2024

**Основные задачи:**

1.1 Список тикеров и групп активов
   - Определён перечень активов, включающий:
     - Акции (Equities): Тикеры крупнейших компаний, таких как Apple (AAPL), Microsoft (MSFT), Tesla (TSLA) и др.
     - Сырьевые товары (Commodities): Тикеры для нефти, золота, серебра и других сырьевых фьючерсов.
     - Индексы (Indices): Ключевые рыночные индексы (например, S&P 500, NASDAQ, и индекс волатильности VIX).
     - Облигации (Bond Indices): Доходности государственных облигаций США разного срока.
     - Валюты (Currencies): Валютные пары с долларом США.
   - Все активы разделены на группы для дальнейшего анализа.

1.2 Загрузка данных
   - Источник данных: Yahoo Finance.
   - Загружены исторические данные с 1 января 1990 года по текущий день с интервалом в один день.
   - Для каждого актива добавлены такие столбцы, как 'Date', 'Ticker', 'Company Name', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close'.
   - Создан словарь с полными названиями компаний по тикерам для удобства работы с данными.

1.3 Первичный анализ структуры данных
   - Проверка наличия и структуры колонок: Убедились, что все активы имеют одинаковые колонки, необходимые для дальнейшего анализа.
   - Проверка типов данных: Убедились, что типы данных корректны и готовы к обработке.
   - Проверка частоты и пропусков: Выявлены возможные пропуски данных, которые будут учтены на следующих этапах.

1.4 Разделение на группы активов
   - Активы классифицированы по группам (Equities, Commodities, Indices, Bond Indices, Currencies) для облегчения анализа.

1.5 Очистка данных от выбросов
   - Используется статистика (например, 0.1% и 99.9% перцентили), чтобы определить экстремальные выбросы в данных.
   - Созданы словари для хранения как очищенных данных, так и выбросов.
   - Выведена информация о выбросах и очищенных данных для каждой группы активов.

1.6 Визуализация трендов
   - Построены графики трендов, средних значений и стандартного отклонения для каждого актива.
   - Проведена мультипликативная декомпозиция временного ряда для выявления трендов и сезонных составляющих.
   - Использована модель Prophet для прогнозирования данных, включая задание параметров модели и пользовательской сезонности. Построен прогноз на 365 дней вперед для каждого актива.

1.7 Заполнение пропусков
   - Создана функция для проверки наличия пропусков.
   - Пропуски заполнены методом линейной интерполяции, а оставшиеся пропуски (в начале или конце данных) заполнены методом ближайшего соседа.
   - Проверено отсутствие пропусков после заполнения.

1.8 Ресемплирование данных
   - Построены графики данных с разными интервалами ресемплирования для выявления сезонности и тенденций с разными временными интервалами.

1.9 Статистический анализ
   - Вычислены статистические характеристики для каждого актива.
   - Построена корреляционная тепловая карта для визуализации взаимосвязей между активами.

1.10 Тесты временных рядов на стационарность (ADF тест)
   - Проведен тест ADF (Augmented Dickey-Fuller) на стационарность для каждого актива.
   - Определено, стационарен ли временной ряд, что важно для последующего моделирования.

1.11 Анализ автокорреляции (ACF и PACF)
   - Проверка на автокорреляцию с использованием графиков ACF и PACF.
   - Анализ позволяет выявить сезонные паттерны и задержки, которые могут быть полезны для дальнейшего моделирования.

1.12 Обработка нестационарных рядов
   - Применено логарифмирование данных и тесты на стационарность для временных рядов.
   - Использованы скользящие метрики и графики автокорреляции для анализа после логарифмирования.
   - Проведено дифференцирование для достижения стационарности.

1.13 Поквартальный анализ
   - Проведен поквартальный анализ для выявления сезонных трендов и циклов.
   - Данные анализируются поквартально для каждой группы активов, что позволяет сравнить их влияние на общие рыночные тренды.

1.14 Загрузка макроэкономических данных
   - Использован API FRED для загрузки ключевых макроэкономических показателей (например, ВВП, безработица, CPI, процентные ставки).
   - Данные агрегированы поквартально для сравнения с поквартальными данными активов.

1.15 Анализ макроэкономических данных
   - Определены долгосрочные тренды и отклонения в макроэкономических показателях.
   - Для каждого показателя построены графики, на которых можно увидеть влияние мировых кризисов на экономику.

1.16 Тесты на стационарность для макроэкономических показателей
   - Проведены тесты ADF и KPSS для выявления стационарности макроэкономических показателей.
   - Определены ряды, требующие преобразования для дальнейшего анализа.

1.17 Преобразование данных для достижения стационарности
   - Применено разностное преобразование для достижения стационарности.
   - Получены скорректированные данные, которые позволяют анализировать изменения в динамике активов и макроэкономических показателей без учёта тренда и сезонности.

1.18 Финальная аналитика и построение корреляционных матриц:
   - Данные активов и макроэкономических показателей приведены к общему формату и нормализованы для анализа корреляции.
   - Построены корреляционные матрицы для выявления взаимосвязей между рыночными активами и макроэкономическими показателями, что позволяет делать выводы о влиянии макроэкономических факторов на рыночные активы.

1.19 Выполнен дополнительный пункт - анализ влияния. 

### 2. Разработка моделей ARIMA и Exponentioal Smoothing
**Ответственные:** Все участники команды (разделение задач по моделям)
**Сроки:** Декабрь 2024

**Основные задачи:**

2.1 Обработка данных для моделей:
- Проведен анализ данных с 1004 тикерами, содержащих более 1 млн значений (цены, объемы и др.).
- Удалены выбросы и недостающие значения.  
- Создание временных лагов для использования в моделях временных рядов.
- Построены графики реальных и прогнозных значений, а также графики остатков для оценки точности моделей.

2.2 Разработка моделей:
- Преобразованы временные ряды (цены `adjusted_close`) для проверки и достижения стационарности (дифференцирование, ADF-тест).  
- Определены параметры (p, d, q) с помощью ACF и PACF. Построены и обучены модели ARIMA для каждого тикера.  
- Оценена корректность моделей (анализ остатков: нормальность, отсутствие автокорреляции, визуализация).
- Выполнено краткосрочное прогнозирование дифференцированных данных, преобразованных обратно для интерпретации.  
     
2.3 Метрики и подбор гиперпараметров:
- Оценка моделей по метрикам RMSE, MSE, MAE, и кросс-валидация для повышения устойчивости.

2.4 Сравнение моделей и выводы:
- Сравнение моделей между собой и выбор наиболее точной (точных) для дальнейшей работы. 

### 3-4. Разработка и доработка моделей для прогнозирования - ARIMA, SARIMA. Создание сервиса для прогнозирования
**Ответственный:** Все участники команды 
**Сроки:** MVP к декабрю 2024, впоследствии доработка, наполнение и полная реализация к Апрелю – Маю 2025 года

**Основные задачи:** (идеальный вариант реализации, к которому будем стремится)

4.1 Архитектура сервиса
- Проектирование архитектуры сервиса на основе микросервисов с использованием фреймворка FastAPI.
- Подключение моделей для получения прогнозов.
- Проектирование базы данных для хранения исторических данных и результатов прогнозов.

4.2 Реализация API
- Разработка REST API для взаимодействия с моделями (ввод данных, получение прогнозов).
- Реализация эндпоинтов для работы с различными временными рамками (например, дневные, недельные прогнозы).
- Валидация и обработка входящих данных (от пользователей и источников данных).
- Подключение внешних источников для автоматического обновления данных (например, с помощью скриптов для парсинга API).**

4.3 Frontend на Streamlit
- Разработка простого пользовательского интерфейса для визуализации результатов прогнозов.
- Внедрение графиков для отображения исторических данных, текущих трендов и прогнозов.
- Интеграция функций для выбора акций, таймфреймов и отображения моделей с лучшей точностью прогноза.**

4.4 Развертывание и тестирование
- Настройка CI/CD процесса для автоматического тестирования и развертывания сервиса на облачных платформах.
- Организация системы мониторинга производительности моделей (периодическая проверка точности прогнозов, обновление данных и моделей).**

4.5 Дополнительные задачи**:
- Разработан веб-сайт для анализа сентиментов с целью прогнозирования цен. Front-End реализован на JavaScript, Back-End — на Python, размещены на арендованном VPS.
- Back-End обрабатывает данные и предоставляет API, Front-End отвечает за визуализацию результатов.
- Обеспечена высокая производительность и безопасность.
- Внедрение системы управления пользователями и доступа к сервису (данная задача не была реализована).
- Интеграция модели обработки новостных данных и сентимента для более точных прогнозов (данная задача не была реализована).
- Разработка мобильной версии интерфейса для использования на смартфонах (данная задача не была реализована).

**Этапы и сроки работы над веб-сервисом:**
+ Октябрь - Ноябрь 2024: Исследование архитектур для микросервисов, выбор фреймворка (FastAPI, платный хостинг).
+ Ноябрь - Декабрь 2024: Разработка базового REST API (выход MVP).
+ Декабрь 2024: Тестирование API с простыми моделями (модели для прогнозирования).
+ Январь 2025: Внедрение ML и DL-моделей в сервис и улучшение точности прогнозов.
+ Март 2025: Разработка интерфейса для пользователя (веб или мобильное приложение).
+ Май 2025: Финальное тестирование и развертывание сервиса на облачной платформе.


### 5. Разработка моделей глубокого обучения (DL)
**Ответственные:** Все участники команды (разделение задач по моделям)
**Сроки:** Январь 2025 – Март 2025

**Основные задачи:**
3.1 Модели глубокого обучения:
- Построение моделей LSTM и GRU для временных рядов.
- Использование архитектур с учетом многовариантности и сезонности данных.*
- Применение Attention-механизмов для улучшения точности.*

3.2 Оптимизация и настройка моделей:
- Оптимизация структуры сети и подбор гиперпараметров (размеры слоев, количество эпох, шаг обучения).
- Использование методов регуляризации для предотвращения переобучения.

3.3 Сравнение моделей:
- Сравнение моделей глубокого обучения с ML моделями по точности и эффективности.
- Пайплайн для создания ансамблей моделей для улучшения предсказательных способностей.


### 6. Финальная подготовка проекта и защита
**Ответственные:** Все участники команды
**Сроки:** Май-Июнь 2025

**Основные задачи:**
- Презентация финальных результатов (модели, сервис, визуализация).
- Оценка результатов работы моделей и точности прогнозов.
- Подготовка демонстрационного сценария работы сервиса.
- Финальное тестирование и улучшение сервиса.
- Написание и публикация статьи по проделанной работе / о полученных результатах
